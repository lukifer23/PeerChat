# PeerChat

PeerChat is an on-device AI chat application for Android that runs entirely offline using llama.cpp with Vulkan acceleration. It supports local GGUF models, RAG-based document search, and provides a production-ready architecture for privacy-conscious AI interaction.

## Features

- **On-device inference**: Zero network traffic, all processing runs locally on your device
- **Vulkan acceleration**: GPU-accelerated inference for improved performance
- **RAG support**: Document ingestion and semantic search with embedding-based retrieval
- **Streaming responses**: Real-time token streaming with performance metrics
- **Room database**: Persistent chat history with full-text search capabilities
- **Model management**: Support for both sideloaded and downloadable GGUF models

## Architecture

The project is organized as a multi-module Android application:

- **app**: Main application module with Compose UI and chat interface
- **engine**: Native inference engine wrapping llama.cpp with JNI bindings
- **data**: Room database layer for persistence (chats, messages, documents, embeddings)
- **rag**: RAG service for document indexing and semantic retrieval
- **templates**: Model template definitions
- **docs**: Documentation and resources

## Requirements

- Android SDK 26+ (Android 8.0)
- NDK with CMake 3.22.1+
- Vulkan-capable device (ARM64-v8a)
- Kotlin 1.9.24

## Building

From the project root:

```bash
./gradlew :app:assembleRelease
```

For debug builds:

```bash
./gradlew :app:assembleDebug
```

Install to device:

```bash
adb install app/build/outputs/apk/debug/app-debug.apk
```

## Native Engine

The engine module wraps llama.cpp with the following configuration:

- **GGML_USE_VULKAN**: Vulkan backend for GPU acceleration
- **GGML_VULKAN_CHECK_RESULTS**: Validation mode for debugging
- **GGML_USE_K_QUANTS**: Quantized model support

Build the native library:

```bash
cd engine
./gradlew :engine:assembleRelease
```

## Model Support

PeerChat supports GGUF format models with Q4_K_M quantization recommended for optimal performance. Default models are documented in `defaultmodels.md`.

Place GGUF models in the app's private directory or configure external storage paths in model settings.

## RAG Pipeline

The RAG service provides:

- Document ingestion with PDF and text file support
- Tokenizer-aware chunking with configurable overlap
- Cosine similarity-based retrieval
- Hybrid search combining semantic and full-text matching

Documents are indexed with embeddings generated by the inference engine and stored in the Room database for efficient retrieval.

## Data Model

Core entities:

- **Folder**: Chat organization and categorization
- **Chat**: Conversation thread with model configuration and settings
- **Message**: Individual messages with markdown content and performance metrics
- **Document**: Imported files for RAG indexing
- **Embedding**: Vector embeddings for semantic search
- **RagChunk**: Text chunks extracted from documents

## Performance Metrics

The engine tracks comprehensive metrics for each generation:

- **TTFS**: Time to first token
- **TPS**: Tokens per second (generation speed)
- **Context utilization**: Percentage of context window used
- **Prefill/Decode timing**: Breakdown of pipeline stages

## Development Status

This is an active development project. See `on.plan.md` for the current roadmap and workstream status. Key areas in progress:

- Native engine JNI hardening and KV reuse support
- Model lifecycle management and manifest system
- Navigation Compose architecture and ViewModel-backed state
- Enhanced RAG pipeline with ANN index persistence
- Security and privacy hardening

## License

MIT License - see LICENSE file for details.

The project includes llama.cpp as a submodule, which is also licensed under MIT.

